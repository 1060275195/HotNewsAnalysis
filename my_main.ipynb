{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from utils import news_crawler\n",
    "from utils import preprocessing\n",
    "from utils import modeling\n",
    "from utils import counter\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "__file__ = r'E:\\Projects\\SpyderProjects\\HotNewsAnalysis\\main.ipynb'\n",
    "# 获取项目路径\n",
    "project_path = os.path.dirname(os.path.realpath(__file__))\n",
    "# 获取数据存放目录路径\n",
    "data_path = os.path.join(project_path, 'data')\n",
    "news_path = os.path.join(data_path, 'news')\n",
    "extra_dict_path = os.path.join(data_path, 'extra_dict')\n",
    "fonts_path = os.path.join(data_path, 'fonts')\n",
    "results_path = os.path.join(data_path, 'results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sina_news_df = news_crawler.get_latest_news('sina', top=1000, show_content=True)\n",
    "# sohu_news_df = news_crawler.get_latest_news('sohu', top=1000, show_content=True)\n",
    "# xinhuanet_news_df = news_crawler.get_latest_news('xinhuanet', top=100, show_content=True)\n",
    "# news_crawler.save_news(sina_news_df, os.path.join(news_path, 'sina_latest_news.csv'))\n",
    "# news_crawler.save_news(sohu_news_df, os.path.join(news_path, 'sohu_latest_news.csv'))\n",
    "# news_crawler.save_news(xinhuanet_news_df, os.path.join(news_path, 'xinhuanet_latest_news.csv'))\n",
    "# news_crawler.threaded_crawler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sina_news_df = news_crawler.load_news(os.path.join(news_path, 'sample_sina_latest_news.csv'))\n",
    "sohu_news_df = news_crawler.load_news(os.path.join(news_path, 'sample_sohu_latest_news.csv'))\n",
    "xinhuanet_news_df = news_crawler.load_news(os.path.join(news_path, 'sample_xinhuanet_latest_news.csv'))\n",
    "news_df = pd.concat([sina_news_df, sohu_news_df, xinhuanet_news_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape= (2619, 4)\n"
     ]
    }
   ],
   "source": [
    "df = preprocessing.data_filter(news_df)\n",
    "# now_time = datetime.strftime(datetime.now(), '%Y-%m-%d %H:%M')\n",
    "now_time = '2018-04-06 23:59'\n",
    "df = preprocessing.get_data(df, last_time=now_time, delta=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 新闻标题聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Jacen\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.123 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "df_title = df.copy()\n",
    "df_title['title_'] = df_title['title'].map(lambda x: preprocessing.clean_title(x))\n",
    "df_title['title_'] = df_title['title_'].map(lambda x: preprocessing.get_num_en_ch(x))\n",
    "df_title['title_cut'] = df_title['title_'].map(lambda x: preprocessing.pseg_cut(\n",
    "    x, userdict_path=os.path.join(extra_dict_path, 'self_userdict.txt')))\n",
    "df_title['title_cut'] = df_title['title_cut'].map(lambda x: preprocessing.get_words_by_flags(\n",
    "    x, flags=['n.*', '.*n', 'v.*', 's', 'j', 'l', 'i', 'eng']))\n",
    "df_title['title_cut'] = df_title['title_cut'].map(lambda x: preprocessing.stop_words_cut(\n",
    "    x, os.path.join(extra_dict_path, 'HIT_stop_words.txt')))\n",
    "df_title['title_cut'] = df_title['title_cut'].map(lambda x: preprocessing.stop_words_cut(\n",
    "    x, os.path.join(extra_dict_path, 'self_stop_words.txt')))\n",
    "df_title['title_cut'] = df_title['title_cut'].map(lambda x: preprocessing.disambiguation_cut(\n",
    "    x, os.path.join(extra_dict_path, 'self_disambiguation_dict.json')))\n",
    "df_title['title_cut'] = df_title['title_cut'].map(lambda x: preprocessing.individual_character_cut(\n",
    "    x, os.path.join(extra_dict_path, 'self_individual_character_dict.txt')))\n",
    "df_title['title_'] = df_title['title_cut'].map(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_library_list = counter.get_word_library(df_title['title_cut'])\n",
    "single_frequency_words_list = counter.get_single_frequency_words(df_title['title_cut'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3844"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features = len(word_library_list) - len(single_frequency_words_list) // 2\n",
    "max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2619, 3844)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_matrix = modeling.feature_extraction(df_title['title_'], vectorizer='CountVectorizer',\n",
    "                                           vec_args={'max_df': 1.0, 'min_df': 1, 'max_features': max_features})\n",
    "title_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_dbscan = modeling.get_cluster(title_matrix, cluster='DBSCAN',\n",
    "                                    cluster_args={'eps': 0.4, 'min_samples': 4, 'metric': 'cosine'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({-1: 2341,\n",
       "         0: 150,\n",
       "         1: 7,\n",
       "         2: 8,\n",
       "         3: 4,\n",
       "         4: 4,\n",
       "         5: 4,\n",
       "         6: 8,\n",
       "         7: 4,\n",
       "         8: 4,\n",
       "         9: 8,\n",
       "         10: 5,\n",
       "         11: 4,\n",
       "         12: 4,\n",
       "         13: 14,\n",
       "         14: 4,\n",
       "         15: 5,\n",
       "         16: 4,\n",
       "         17: 4,\n",
       "         18: 5,\n",
       "         19: 5,\n",
       "         20: 4,\n",
       "         21: 5,\n",
       "         22: 6,\n",
       "         23: 4,\n",
       "         24: 4})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_labels = modeling.get_labels(title_dbscan)\n",
    "df_title['title_label'] = title_labels\n",
    "title_rank = modeling.label2rank(title_labels)\n",
    "df_title['title_rank'] = title_rank\n",
    "Counter(title_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_label_num = counter.get_num_of_value_no_repeat(title_labels)\n",
    "title_label_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['美国', '中国', '关税', '加征', '特朗普', '贸易战', '商品', '对华', '产品', '建议']\n"
     ]
    }
   ],
   "source": [
    "df_ = df_title[df_title['title_rank'] == 1]\n",
    "title_top_list = counter.get_most_common_words(df_['title_cut'], top_n=10)\n",
    "# print(df_)\n",
    "print(title_top_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 新闻内容聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_content = df.copy()\n",
    "df_content['content_'] = df_content['content'].map(lambda x: preprocessing.clean_content(x))\n",
    "df_content['content_'] = df_content['content_'].map(lambda x: preprocessing.get_num_en_ch(x))\n",
    "df_content['content_cut'] = df_content['content_'].map(lambda x: preprocessing.pseg_cut(\n",
    "    x, userdict_path=os.path.join(extra_dict_path, 'self_userdict.txt')))\n",
    "df_content['content_cut'] = df_content['content_cut'].map(lambda x: preprocessing.get_words_by_flags(\n",
    "    x, flags=['n.*', '.*n', 'v.*', 's', 'j', 'l', 'i', 'eng']))\n",
    "df_content['content_cut'] = df_content['content_cut'].map(lambda x: preprocessing.stop_words_cut(\n",
    "    x, os.path.join(extra_dict_path, 'HIT_stop_words.txt')))\n",
    "df_content['content_cut'] = df_content['content_cut'].map(lambda x: preprocessing.stop_words_cut(\n",
    "    x, os.path.join(extra_dict_path, 'self_stop_words.txt')))\n",
    "df_content['content_cut'] = df_content['content_cut'].map(lambda x: preprocessing.disambiguation_cut(\n",
    "    x, os.path.join(extra_dict_path, 'self_disambiguation_dict.json')))\n",
    "df_content['content_cut'] = df_content['content_cut'].map(lambda x: preprocessing.individual_character_cut(\n",
    "    x, os.path.join(extra_dict_path, 'self_individual_character_dict.txt')))\n",
    "df_content['content_'] = df_content['content_cut'].map(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_library_list = counter.get_word_library(df_content['content_cut'])\n",
    "single_frequency_words_list = counter.get_single_frequency_words(df_content['content_cut'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49163 19390 39468\n"
     ]
    }
   ],
   "source": [
    "max_features = len(word_library_list) - len(single_frequency_words_list) // 2\n",
    "print(len(word_library_list), len(single_frequency_words_list), max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2619, 39468)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_matrix = modeling.feature_extraction(df_content['content_'], vectorizer='CountVectorizer',\n",
    "                                             vec_args={'max_df': 0.95, 'min_df': 1, 'max_features': max_features})\n",
    "content_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_dbscan = modeling.get_cluster(content_matrix, cluster='DBSCAN',\n",
    "                                      cluster_args={'eps': 0.35, 'min_samples': 4, 'metric': 'cosine'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({-1: 1872,\n",
       "         0: 385,\n",
       "         1: 10,\n",
       "         2: 30,\n",
       "         3: 11,\n",
       "         4: 15,\n",
       "         5: 4,\n",
       "         6: 13,\n",
       "         7: 7,\n",
       "         8: 17,\n",
       "         9: 43,\n",
       "         10: 5,\n",
       "         11: 5,\n",
       "         12: 4,\n",
       "         13: 9,\n",
       "         14: 4,\n",
       "         15: 4,\n",
       "         16: 5,\n",
       "         17: 6,\n",
       "         18: 12,\n",
       "         19: 10,\n",
       "         20: 4,\n",
       "         21: 6,\n",
       "         22: 8,\n",
       "         23: 4,\n",
       "         24: 4,\n",
       "         25: 4,\n",
       "         26: 10,\n",
       "         27: 4,\n",
       "         28: 10,\n",
       "         29: 4,\n",
       "         30: 4,\n",
       "         31: 4,\n",
       "         32: 5,\n",
       "         33: 4,\n",
       "         34: 4,\n",
       "         35: 8,\n",
       "         36: 9,\n",
       "         37: 4,\n",
       "         38: 5,\n",
       "         39: 7,\n",
       "         40: 4,\n",
       "         41: 5,\n",
       "         42: 4,\n",
       "         43: 5,\n",
       "         44: 5,\n",
       "         45: 4,\n",
       "         46: 5,\n",
       "         47: 4})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_labels = modeling.get_labels(content_dbscan)\n",
    "df_content['content_label'] = content_labels\n",
    "content_rank = modeling.label2rank(content_labels)\n",
    "df_content['content_rank'] = content_rank\n",
    "Counter(content_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['企业', '市场', '上市', 'cdr', 'a股', '发行', '独角兽', '试点', '创新', '经济', '估值', '存托', '公司', '凭证', '境内']\n"
     ]
    }
   ],
   "source": [
    "df_ = df_content[df_content['content_rank'] == 2]\n",
    "content_top_list = counter.get_most_common_words(df_['content_cut'], top_n=15, min_frequency=1)\n",
    "# print(df_)\n",
    "print(content_top_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 综合分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_title = news_crawler.load_news(os.path.join(results_path, 'df_title_rank.csv'))\n",
    "# df_content = news_crawler.load_news(os.path.join(results_path, 'df_content_rank.csv'))\n",
    "# df_title['title_cut'] = df_title['title_cut'].map(eval)\n",
    "# df_content['content_cut'] = df_content['content_cut'].map(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_title_content = df_title.copy()\n",
    "df_title_content['content_cut'] = df_content['content_cut']\n",
    "df_title_content['content_rank'] = df_content['content_rank']\n",
    "df_title_content = modeling.get_non_outliers_data(df_title_content, label_column='title_rank')\n",
    "title_rank_num = counter.get_num_of_value_no_repeat((df_title_content['title_rank']))\n",
    "title_rank_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "热点： 中美贸易战再升级：特朗普考虑对1000亿美元中国商品加征关税 \n",
      "\n",
      "相关词汇： ['美国', '中国', '关税', '贸易', '产品', '加征', '特朗普', '措施', '中美', '经济', '贸易战', '表示', '商品', '出口', '进口', '制造', '大豆', '国家', '公布', '调查']\n",
      "相关词汇： ['美国', '用于', '超过', '中国', '产品', '包括', '机械', '合金钢', 'mm', '部件', '设备', '关税', '加工', '机器', '金属', '仪器', '装置', '附件', '发动机', '猪肉']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "热点： 钜盛华拟清算持股万科资管计划 稳定股价成首要考虑 \n",
      "\n",
      "相关词汇： ['万科', '计划', '资管', '钜盛华', '股份', '持有', '宝能', '管理', '持股', '相关', '市场', '减持', '清算', '转让', '交易', '处置', '股东', '资产', '协议', '股价']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "热点： 中央财经委首提结构性去杠杆 一行两会准备怎么干 \n",
      "\n",
      "相关词汇： ['杠杆', '攻坚战', '去杠杆', '打好', '政府', '金融风险', '中央财经委员会', '发展', '地方', '风险', '会议', '防范', '问题', '部门', '结构性', '化解', '工作', '坚持', '经济', '提出']\n",
      "相关词汇： ['杠杆', '去杠杆', '企业', '结构性', '政府', '债务', '会议', '地方', '提出', '中央财经委员会', '国有企业', '金融', '部门', '上升', '工作', '中国', '经济', '负债', '融资', '表示']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "热点： 亚太股市周三涨跌不一 日经指数小幅走低 \n",
      "\n",
      "相关词汇： ['美国', '指数', '中国', '建议', '产品', '时间', '征收', '关税']\n",
      "相关词汇： ['指数', '美国', '中国', '科技股', '关税', '市场', '贸易战', '措施', '纳指', '开盘', '韩国', '中美', '贸易', '升级', '澳大利亚', '标普', '北京', '时间', '消息', '股市']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "热点： 469亿资金争夺20股：主力资金重点出击8股(名单) \n",
      "\n",
      "相关词汇： ['主力', '净额', '资金', '流出', '流入', '股票', '市场', '收盘', '呈现', '有所', '新浪财经', '影响', '沪指', '指数', '情绪', '三大股指', '创指', '临近', '显示', '流向']\n",
      "相关词汇： ['项目', '股份', '投资', '京报', '公司', '安吉', '中国', '资金', '表示', '法门寺', '违约', '港桥', '集团', '新奇', '市场', '王永红', '浙江', '建设', '负责人', '央企']\n",
      "相关词汇： ['资金', '指数', '主力', '流出', '市场', '行业', '临近', '人士', '表示', '外流', '数据', '金额', 'a股', 'msci', '成分股', '业绩', '布局', '震荡', '创业板', '长假']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "热点： 韩国前总统朴槿惠一审获刑24年 \n",
      "\n",
      "相关词汇： ['朴槿惠', '韩国', '总统', '崔顺', '亲信', '检方', '宣判', '法院', '涉嫌', '中央', '地方法院', '进行', '一审', '接受', '直播', '干政门', '干政', '调查', '指控', '青瓦台']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "热点： 特朗普一周四次炮轰亚马逊 \n",
      "\n",
      "相关词汇： ['亚马逊', '特朗普', '美国', '邮政', '表示', '机构', '监管', '公司', '服务', '炮轰', '总统', '支付', '产品', '股价', '不会', '采取', '认为', '改变', '收购', '导致']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "热点： 黄金股午后再次活跃 西部黄金涨逾8% \n",
      "\n",
      "相关词汇： ['黄金']\n",
      "相关词汇： ['黄金', '市场']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "热点： 证监会：妥善处置风险隐患 全力维护市场稳健运行 \n",
      "\n",
      "相关词汇： ['会议', '金融风险', '防范', '风险', '化解', '习近平']\n",
      "相关词汇： ['资本', '市场', '会议', '工作', '全面', '党委', '深化改革', '改革', '同志', '中央', '经济', '党中央', '金融', '加强', '发展', '坚持', '委员会', '精神', '习近平', '要求']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "热点： 开盘：三大股指高开沪指涨0.03% 宁德时代概念股大涨 \n",
      "\n",
      "相关词汇： ['市场', '中国', '美国', '科技', '股份', '个股', '榜前列', '企业', '认为', '创业板', '沪指', '盘面', '互联网']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "热点： 知情人士：阿里全资收购饿了么落定 今天或内部宣布 \n",
      "\n",
      "相关词汇： ['饿了么', '阿里巴巴', '外卖', '阿里', '美团', '收购', '服务', '零售', '张旭', '市场', '生活', '配送', 'ceo', '交易', '百度', '公司', '口碑', '点评', '表示', '创始人']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "热点： 瑞斯康达：股价连续四日涨停 明起停牌核查 \n",
      "\n",
      "相关词汇： ['停牌', '消息', '核查']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "热点： 欧元、英镑、澳元、黄金、原油技术分析与预测 \n",
      "\n",
      "相关词汇： ['黄金', '美元', '原油', '现货', '支撑', '区域', '分析', 'fx168', '美国', '走势', '预期', '市场', '阻力', '价格']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "热点： 监管出重拳 非法互联网资管6月底前清零 \n",
      "\n",
      "相关词汇： ['业务', '互联网', '资产', '管理', '网贷', '平台', '资管', '机构', '验收', '备案', '整治', '金融', '通知', '监管', '存量', '产品', '相关', '不得', '发行', '开展']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "热点： 九鼎复牌遭\"两腰斩\" 建信基金等定增基金已浮亏六成 \n",
      "\n",
      "相关词汇： ['基金', '投资', '产品', '规模', '策略', '业绩', '市场', '评价', '分析', '九鼎', '风险', '增长', '货币基金', '投资者', '收益', '进行', '计划', '行业', '股票', '公司']\n",
      "相关词汇： ['基金', '募集', 'msci', '成立', '发行', '型基金', 'a股', '认购', '指数', '新发', '混合型', '中国', '份额', '金额', '主题']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "热点： 白宫经济顾问：预计中美两国将达成贸易协议 \n",
      "\n",
      "相关词汇： ['美国', '表示', '特朗普', '经济顾问', '首席', '贸易', '中美', '德洛', '贸易摩擦', '时间', '接受', '采访', '关税', '预计', '达成协议', '总统', 'kudlow', '两国', '中国']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "热点： 任泽平：中美强硬试探对方底牌 未来形势存两种情景 \n",
      "\n",
      "相关词汇： ['美国', '中国', '中美', '贸易战', '贸易', '经济', '升级', '特朗普', '双方', '对华', '出口', '试探', '扩大', '市场', '金融', '关税', '影响', '公布', '后续', '实力']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "热点： 宁德时代IPO顺利过会 独角兽携手云计算第一股登台 \n",
      "\n",
      "相关词汇： ['宁德时代', '公司', '企业', '股份', '过会', 'ipo', '独角兽', 'sz', '收入', '披露', '显示', '营业', '保荐', '新能源', 'a股', '全球', '星云', '供应商', '集团', '证券']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "热点： 安邦真实资本金10.96亿，保险保障基金注资608亿，启动战投遴选 \n",
      "\n",
      "相关词汇： ['保险', '安邦', '基金', '保障', '集团', '风险', '股东', '处置', '注资', '救助', '引入', '增资', '中华', '公司', '工作', '股权', '联合', '银保监会', '人寿', '保单']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "热点： 4月4日上市公司晚间公告速递 \n",
      "\n",
      "相关词汇： ['公司', '股份', '净利润', '年报', '实现', '股东', '营业', '净利', '收入', '披露', '上市公司', '合作', '协议', '项目', '集团', '收益', '增长', '收购', '股权', '领域']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "热点： 环保板块逆势拉升 中环环保涨停 \n",
      "\n",
      "相关词汇： ['环保']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "热点： 悬疑新剧情 乐视网是否会“空壳化”再引深交所问询 \n",
      "\n",
      "相关词汇： ['乐视网', '新乐视智家', '公司', '乐视', '质押', '融创', '股权', '影业', '孙宏斌', '腾讯', '无法', '成为', '股东', '资产', '显示', '有限公司', '出现', '偿债', '失去', '投资']\n",
      "相关词汇： ['检查', '现场', '监管', '深交所', '上市公司', '交易所', '相关', '证监局', '表示', '合规', '负责人', '市场', '风险', '部门', '公司', '工作', '检查部', '联合', '进行', '参与']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "热点： 3月物价涨幅或现明显回调 \n",
      "\n",
      "相关词汇： ['价格', '回落', '月份', '食品', 'cpi', '表示', '猪价', '物价', '首席', '国内', '出现', '幅度', '预计', '因素', '猪肉', '生猪', '亏损', '需求', '波动', '经济学家']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "热点： 上海莱士不务正业热衷炒股 一季度亏9亿赔掉去年利润 \n",
      "\n",
      "相关词汇： ['上海', '莱士', '炒股', '公司', '万丰', '奥威', '业绩', '净利润', '显示', '兴源', '环境', '投资', '披露', '亏损', '持有', '计划', '股票', '信托', '停牌', '股东']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "热点： 机器人概念股走强 三丰智能涨停 \n",
      "\n",
      "相关词汇： ['智能', '股份', '机器人', '概念股', '三丰']\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, title_rank_num + 1):\n",
    "    df_i = df_title_content[df_title_content['title_rank'] == i]\n",
    "    title = '\\n'.join(df_i['title'].tolist())\n",
    "    title = modeling.get_key_sentences(title, num=1)\n",
    "    print('热点：', title, '\\n')\n",
    "    content_rank = [k for k in df_i['content_rank']]\n",
    "    content_rank = set(content_rank)\n",
    "    for j in content_rank:\n",
    "        df_j = df_i[df_i['content_rank'] == j]\n",
    "        most_commmon_words = counter.get_most_common_words(df_j['content_cut'], top_n=20, min_frequency=5)\n",
    "        if len(most_commmon_words) > 0:\n",
    "            print('相关词汇：', most_commmon_words)\n",
    "    print('-' * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
